[
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model",
    "section": "",
    "text": "Let’s make a predictive model based on the palmer penguins dataset. First we’ll load in the packages we want.\nCode\nfrom palmerpenguins import penguins\nfrom pandas import get_dummies\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\n\nfrom vetiver import VetiverModel\nfrom vetiver import VetiverAPI\n\nimport pins\nfrom vetiver import vetiver_pin_write\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nimport duckdb"
  },
  {
    "objectID": "model.html#get-data",
    "href": "model.html#get-data",
    "title": "Model",
    "section": "Get Data",
    "text": "Get Data\nHere, we pull our data from an existing DuckDB database, removing rows with missing data.\n\n\nCode\ncon = duckdb.connect('my-db.duckdb')\ndf = con.execute(\"SELECT * FROM penguins\").fetchdf().dropna()\ncon.close()"
  },
  {
    "objectID": "model.html#define-model-and-fit",
    "href": "model.html#define-model-and-fit",
    "title": "Model",
    "section": "Define Model and Fit",
    "text": "Define Model and Fit\nLet’s start by fitting a simple linear regression model in python.\n\n\nCode\nX = get_dummies(df[['bill_length_mm', 'species', 'sex']], drop_first = True)\ny = df['body_mass_g']\n\nmodel = LinearRegression().fit(X, y)"
  },
  {
    "objectID": "model.html#get-some-information",
    "href": "model.html#get-some-information",
    "title": "Model",
    "section": "Get some information",
    "text": "Get some information\nWe can also get statistics to evaluate the model\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\n# ... (assuming the model and the variable 'X' are already defined and available)\n\n# Create a DataFrame to display the model's statistics\nmodel_stats = pd.DataFrame({\n    'Metric': ['R^2', 'Intercept', 'Coefficients', 'RMSE'],\n    'Value': [\n        model.score(X, y),\n        model.intercept_,\n        model.coef_,\n        np.sqrt(mean_squared_error(y, model.predict(X)))\n    ]\n})\n\n# Use the `to_html` method to render the DataFrame as an HTML table\n# Set `escape=False` to allow HTML elements within the table\nhtml_output = model_stats.to_html(index=False, escape=False)\n\n# Optionally, apply additional styling to the table using the Styler object\nstyled_output = model_stats.style.set_table_attributes('style=\"width:100%; border:1px solid black;\"')\n\nstyled_output\n\n\n\n\n\n\n\n \nMetric\nValue\n\n\n\n\n0\nR^2\n0.855537\n\n\n1\nIntercept\n2169.269721\n\n\n2\nCoefficients\n[ 32.53688677 -298.76553447 1094.86739145 547.36692408]\n\n\n3\nRMSE\n305.588999"
  },
  {
    "objectID": "model.html#what-if-we-tried-some-other-models",
    "href": "model.html#what-if-we-tried-some-other-models",
    "title": "Model",
    "section": "What if we tried some other models?",
    "text": "What if we tried some other models?\nLinear models are great, but they don’t always capture the full relationship. Let’s start by splitting the data into a test and training set. It’s better practice than training on all your data. We’ll start with the same linear model, but trained on the training set and tested on the test set. Then we’ll play with the number of predictors and try some more advanced models.\n\n\nCode\n# Step 1: Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 2: Train the model\n# We can't just train the model on all our data. What do we look like, savages?\n# \nmodel1 = LinearRegression()\nmodel1.fit(X_train, y_train)\n\n# Step 3: Make predictions on the testing set\ny_pred1 = model1.predict(X_test)\n\n# Step 4: Calculate RMSE\nrmse = np.sqrt(mean_squared_error(y_test, y_pred1))\n\nprint(f\"RMSE: {rmse}\")\n\n\nRMSE: 310.2846345747236\n\n\nIt’s worse =( It’s possible the first model overfit, but hard to say for sure.\n\nWhat if we did more predictors?\n\n\nCode\n# Let's try all the variables\nX = pd.get_dummies(df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'year', 'species', 'island', 'sex']], drop_first=True)\ny = df['body_mass_g']\n\n# Step 1: Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nmodel2 = LinearRegression()\nmodel2.fit(X_train, y_train)\n\n# Step 3: Make predictions on the testing set\ny_pred2 = model2.predict(X_test)\n\n# Step 4: Calculate RMSE\nrmse = np.sqrt(mean_squared_error(y_test, y_pred2))\n\nprint(f\"RMSE: {rmse}\")\n\n\nRMSE: 275.14079947919276\n\n\nThis one is much better.\n\n\nWhat about other models?\nLet’s try a decision tree\n\n\nCode\n# For classification\ntree_model = DecisionTreeRegressor()\ntree_model.fit(X_train, y_train)\ny_pred_tree = tree_model.predict(X_test)\n\n\n# Step 4: Calculate RMSE\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n\nprint(f\"RMSE: {rmse}\")\n\n\nRMSE: 413.6319836647642"
  },
  {
    "objectID": "model.html#lets-tune-some-hyper-parameters",
    "href": "model.html#lets-tune-some-hyper-parameters",
    "title": "Model",
    "section": "Let’s tune some hyper parameters!",
    "text": "Let’s tune some hyper parameters!\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the model\ntree_model = DecisionTreeRegressor()\n\n# Define the parameters grid\nparam_grid = {\n    'max_depth': [3, 5, 7, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n}\n\n# Perform grid search\ngrid_search = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Get the best model\nbest_tree_model = grid_search.best_estimator_\n\n# Predict and evaluate\ny_pred = best_tree_model.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\n\n\n\nCode\nrmse\n\n\n299.3239266006268"
  },
  {
    "objectID": "model.html#better-but-still-worse-that-linear.-lets-try-a-boosted-random-forest-model.",
    "href": "model.html#better-but-still-worse-that-linear.-lets-try-a-boosted-random-forest-model.",
    "title": "Model",
    "section": "Better, but still worse that linear. Let’s try a boosted random forest model.",
    "text": "Better, but still worse that linear. Let’s try a boosted random forest model.\n\n\nCode\n# Python code for a gradient boosting model using the same features as the linear regression model\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Assuming the dataframe 'df' is already defined and preprocessed\n\n# Define features and target variable\nX = pd.get_dummies(df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'year', 'species', 'island', 'sex']], drop_first=True)\ny = df['body_mass_g']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and fit the Gradient Boosting model\nboosted_model = GradientBoostingRegressor(random_state=42)\nboosted_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred_boosted = boosted_model.predict(X_test)\n\n# Calculate RMSE\nrmse_boosted = np.sqrt(mean_squared_error(y_test, y_pred_boosted))\n\nrmse_boosted\n\n\n284.37462611929544\n\n\nIntriguing. What if we tuned it?\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up the parameter grid to tune\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 4, 5],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 3],\n    'max_features': ['sqrt', 'log2', None]\n}\n\n# Initialize the estimator\ngbr = GradientBoostingRegressor(random_state=42)\n\n# Perform grid search\ngrid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters and the best model\nbest_params = grid_search.best_params_\nbest_gbr_model = grid_search.best_estimator_\n\n\nFitting 5 folds for each of 324 candidates, totalling 1620 fits\n\n\n\n\nCode\n# Assuming the grid search has been performed and the best estimator has been found\n\n# Predict on the test set using the best model\ny_pred_best = best_gbr_model.predict(X_test)\n\n# Calculate RMSE for the best model\nrmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n\nrmse_best\n\n\n276.3223483561153\n\n\nVery close to the linear model with all the predictors. It’s possible that with more tuning we could get the rmse below that of the linear model, but for demonstration purposes, we’ll go with the linear model."
  },
  {
    "objectID": "model.html#saving-and-exporting-the-model",
    "href": "model.html#saving-and-exporting-the-model",
    "title": "Model",
    "section": "Saving and exporting the model",
    "text": "Saving and exporting the model\nLet’s use the ‘vetiver’ package to save the model.\n\n\nCode\nv = VetiverModel(model2, model_name='penguin_model', prototype_data=X)\n\n\nHere we send it to a pin board that allows us to access the model later.\n\n\nCode\n# Create a board that allows pickled models\nboard = pins.board_folder('data/model', allow_pickle_read=True)\n\n# Assuming 'v' is your VetiverModel object\n# Pin the model to the board\nvetiver_pin_write(board, v, 'penguin_model')\n\n\nModel Cards provide a framework for transparent, responsible reporting. \n Use the vetiver `.qmd` Quarto template as a place to start, \n with vetiver.model_card()\nWriting pin:\nName: 'penguin_model'\nVersion: 20240420T160304Z-07ee9\n\n\nFinally, here is an example of how to reload and run the model as a locally hosted website.\n\n\nCode\nb = pins.board_folder('data/model', allow_pickle_read=True)\nv = VetiverModel.from_pin(b, 'penguin_model')\n\n\n\n\nCode\nfrom vetiver import VetiverAPI\napp = VetiverAPI(v, check_prototype = True)\n\n#app.run(port = 8080)"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Penguins EDA",
    "section": "",
    "text": "Code\n# Load the required packages\nlibrary(caret)\nlibrary(caret)\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)\n\n\nLet’s start by looking at the data.\n\n\nCode\ndf &lt;- palmerpenguins::penguins\nkable(head(df), caption = \"Preview of the Dataset\")\n\n\n\nPreview of the Dataset\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\nWe can also examine the summary statistics of the dataset.\n\n\nCode\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n      where(is.numeric), \n      \\(x) mean(x, na.rm = TRUE)\n      )\n    ) %&gt;%\n  knitr::kable(caption = \"Summary of the Dataset\")\n\n\n\nSummary of the Dataset\n\n\n\n\n\n\n\n\n\n\n\nspecies\nsex\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nyear\n\n\n\n\nAdelie\nfemale\n37.25753\n17.62192\n187.7945\n3368.836\n2008.055\n\n\nAdelie\nmale\n40.39041\n19.07260\n192.4110\n4043.493\n2008.055\n\n\nAdelie\nNA\n37.84000\n18.32000\n185.6000\n3540.000\n2007.000\n\n\nChinstrap\nfemale\n46.57353\n17.58824\n191.7353\n3527.206\n2007.971\n\n\nChinstrap\nmale\n51.09412\n19.25294\n199.9118\n3938.971\n2007.971\n\n\nGentoo\nfemale\n45.56379\n14.23793\n212.7069\n4679.741\n2008.069\n\n\nGentoo\nmale\n49.47377\n15.71803\n221.5410\n5484.836\n2008.066\n\n\nGentoo\nNA\n45.62500\n14.55000\n215.7500\n4587.500\n2008.400"
  },
  {
    "objectID": "eda.html#penguin-size-and-mass-by-sex-and-species",
    "href": "eda.html#penguin-size-and-mass-by-sex-and-species",
    "title": "Penguins EDA",
    "section": "",
    "text": "Code\n# Load the required packages\nlibrary(caret)\nlibrary(caret)\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(knitr)\n\n\nLet’s start by looking at the data.\n\n\nCode\ndf &lt;- palmerpenguins::penguins\nkable(head(df), caption = \"Preview of the Dataset\")\n\n\n\nPreview of the Dataset\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\nWe can also examine the summary statistics of the dataset.\n\n\nCode\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n      where(is.numeric), \n      \\(x) mean(x, na.rm = TRUE)\n      )\n    ) %&gt;%\n  knitr::kable(caption = \"Summary of the Dataset\")\n\n\n\nSummary of the Dataset\n\n\n\n\n\n\n\n\n\n\n\nspecies\nsex\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nyear\n\n\n\n\nAdelie\nfemale\n37.25753\n17.62192\n187.7945\n3368.836\n2008.055\n\n\nAdelie\nmale\n40.39041\n19.07260\n192.4110\n4043.493\n2008.055\n\n\nAdelie\nNA\n37.84000\n18.32000\n185.6000\n3540.000\n2007.000\n\n\nChinstrap\nfemale\n46.57353\n17.58824\n191.7353\n3527.206\n2007.971\n\n\nChinstrap\nmale\n51.09412\n19.25294\n199.9118\n3938.971\n2007.971\n\n\nGentoo\nfemale\n45.56379\n14.23793\n212.7069\n4679.741\n2008.069\n\n\nGentoo\nmale\n49.47377\n15.71803\n221.5410\n5484.836\n2008.066\n\n\nGentoo\nNA\n45.62500\n14.55000\n215.7500\n4587.500\n2008.400"
  },
  {
    "objectID": "eda.html#penguin-size-vs-mass-by-species",
    "href": "eda.html#penguin-size-vs-mass-by-species",
    "title": "Penguins EDA",
    "section": "Penguin Size vs Mass by Species",
    "text": "Penguin Size vs Mass by Species\nLet’s make some visuals to better understand the relationship between species, body mass and bill length.\n\n\nCode\ndf %&gt;%\n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\nThat’s very pretty. We also use python to perform similar exploration.For example, how clean is the dataset? Are there a lot of null or missing values?\n\n\nCode\nimport palmerpenguins\nimport pandas as pd\n\ndf = palmerpenguins.load_penguins()\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\n\n\nThat’s a clean dataset! Let’s see what the species and island breakdowns are.\n\n\nCode\npd.DataFrame(df['species'].value_counts()).style.set_table_styles(\n    [{'selector': 'tr:hover',\n      'props': [('background-color', 'yellow')]},\n     {'selector': 'th',\n      'props': [('background-color', 'lightblue'),\n                ('color', 'white')]}]\n).set_caption(\"Penguin Counts by Species\")\n\n\n\n\n\nPenguin Counts by Species\n\n\n \ncount\n\n\nspecies\n \n\n\n\n\nAdelie\n152\n\n\nGentoo\n124\n\n\nChinstrap\n68\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(df['island'].value_counts()).style.set_table_styles(\n    [{'selector': 'tr:hover',\n      'props': [('background-color', 'yellow')]},\n     {'selector': 'th',\n      'props': [('background-color', 'lightblue'),\n                ('color', 'white')]}]\n).set_caption(\"Penguin Counts by Island\")\n\n\n\n\n\nPenguin Counts by Island\n\n\n \ncount\n\n\nisland\n \n\n\n\n\nBiscoe\n168\n\n\nDream\n124\n\n\nTorgersen\n52\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = penguins, aes(x = island, fill = species)) + \n  geom_bar(position = \"stack\") +\n  labs(title = \"Count of Penguin Species by Island\",\n       x = \"Island\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\nNeato! Now let’s see the same breakdown as before, but for island instead of population\n\n\nCode\ndf %&gt;%\n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = island)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\nInteresting. The effect seems less pronounced than for species. Additionally, the way the species are grouped across islands, this graph may just be capturing masked versions of the species’ effect on body mass and bill length. Let’s move on to modelling! By heading over to the model tab, you can step through the process of building and evaluating models."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project is a practical application of DevOps principles within a data science context, following the “DevOps for Data Science” textbook. It involves exploratory data analysis (EDA), model building, and deployment using the Palmer Penguins dataset.\n\n\nThe initial phase involved conducting an EDA on the Palmer Penguins dataset to identify key variables and understand data distributions, providing insights into the data’s underlying patterns and relationships. The model tab of the website walks readers through the EDA process.\n\n\n\nSubsequently, a predictive model was developed to analyze the dataset, focusing on understanding the factors influencing penguin body mass. The modeling process included data preprocessing, feature selection, and model evaluation.\n\n\n\nThe final stage involved deploying the model through a Shiny app using the vetiver package. This deployment demonstrates the operationalization of the model, allowing for interactive user engagement and further exploration of the model’s predictive capabilities.\nThe project exemplifies the integration of data science and DevOps practices, showcasing the end-to-end process from data analysis to model deployment."
  },
  {
    "objectID": "about.html#exploratory-data-analysis",
    "href": "about.html#exploratory-data-analysis",
    "title": "About",
    "section": "",
    "text": "The initial phase involved conducting an EDA on the Palmer Penguins dataset to identify key variables and understand data distributions, providing insights into the data’s underlying patterns and relationships. The model tab of the website walks readers through the EDA process."
  },
  {
    "objectID": "about.html#model-development",
    "href": "about.html#model-development",
    "title": "About",
    "section": "",
    "text": "Subsequently, a predictive model was developed to analyze the dataset, focusing on understanding the factors influencing penguin body mass. The modeling process included data preprocessing, feature selection, and model evaluation."
  },
  {
    "objectID": "about.html#deployment-with-vetiver",
    "href": "about.html#deployment-with-vetiver",
    "title": "About",
    "section": "",
    "text": "The final stage involved deploying the model through a Shiny app using the vetiver package. This deployment demonstrates the operationalization of the model, allowing for interactive user engagement and further exploration of the model’s predictive capabilities.\nThe project exemplifies the integration of data science and DevOps practices, showcasing the end-to-end process from data analysis to model deployment."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Journey of Visualization and Modelling",
    "section": "",
    "text": "Explore the fascinating world of Palmer Penguins through data analysis and interactive modeling. If you’re not interested in that, then enjoy these cute penguin pictures!\n\n \n\n\n\nDelve into our exploratory data analysis and uncover intriguing patterns and relationships within the penguin data.\n\n\n\n\nWe also made an app based on our model! Unfortunately it is hosted locally, so you’ll need to to run it on your own machine to see it for yourself. You can download it from the repository here.\n\n\n\n\nFind out more about the project’s background, the data, and the methodologies on our About page.\nWe invite you to explore, interact, and gain insights from our data-driven journey with the Palmer Penguins."
  },
  {
    "objectID": "index.html#discover-insights",
    "href": "index.html#discover-insights",
    "title": "A Journey of Visualization and Modelling",
    "section": "",
    "text": "Delve into our exploratory data analysis and uncover intriguing patterns and relationships within the penguin data."
  },
  {
    "objectID": "index.html#interactive-modeling",
    "href": "index.html#interactive-modeling",
    "title": "A Journey of Visualization and Modelling",
    "section": "",
    "text": "We also made an app based on our model! Unfortunately it is hosted locally, so you’ll need to to run it on your own machine to see it for yourself. You can download it from the repository here."
  },
  {
    "objectID": "index.html#learn-more",
    "href": "index.html#learn-more",
    "title": "A Journey of Visualization and Modelling",
    "section": "",
    "text": "Find out more about the project’s background, the data, and the methodologies on our About page.\nWe invite you to explore, interact, and gain insights from our data-driven journey with the Palmer Penguins."
  }
]